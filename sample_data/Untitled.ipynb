{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import calendar\n",
    "#import h5netcdf\n",
    "import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "#import s3fs\n",
    "import sys\n",
    "import time\n",
    "import xarray as xr\n",
    "from datetime import datetime as datetime\n",
    "from jinja2 import Template\n",
    "from numpy import array\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "'''\n",
    " NAME: POPBEAST\n",
    " VERSION: 2.1\n",
    " DATA ANALYTICS SPECIALIST - CORE CODE DEVELOPER: Diego Perez (@darkquassar / https://linkedin.com/in/diegope) \n",
    " DATA SCIENTIST - MODEL DEVELOPER: Jonathan Ojeda (https://researchgate.net/profile/Jonathan_Ojeda)\n",
    " DESCRIPTION: A python package to automatically generate gridded climate data for APSIM (to be extended for any crop models)\n",
    " PAPERS OR PROJECTS USING THIS CODE: \n",
    "    1. Ojeda JJ, Eyshi Rezaei E, Remeny TA, Webb MA, Webber HA, Kamali B, Harris RMB, Brown JN, Kidd DB, Mohammed CL, Siebert S, Ewert F, Meinke H (2019) Effects of soil- and climate data aggregation on simulated potato yield and irrigation water demand. Science of the Total Environment. 710, 135589. doi:10.1016/j.scitotenv.2019.135589\n",
    "    2. Ojeda JJ, Perez D, Eyshi Rezaei E (2020) The BestiaPop - A Python package to automatically generate gridded climate data for crop models. APSIM Symposium, Brisbane, Australia.\n",
    "\n",
    " HISTORY: \n",
    "    v0.1 - Created python file\n",
    "    v0.2 - Added numpy series extraction\n",
    "    v0.3 - Using pathlib for cross-platform path compatibility\n",
    "    v1.0 - Added progress bar to download routine\n",
    "    v1.5 - Discarded netCDF4 python package in favor of h5netcdf and xarray for faster slice reads\n",
    "    v1.6 - Implemented data read directly from the Cloud (AWS S3) for faster data loads, improved speed x15\n",
    "    v2.0 - Collection of all variable combinations in final dataframe. Obtaining pseudo-MET df from final df.\n",
    "    v2.1 - Generating final MET file\n",
    "    v2.2 - Adding commandline parameter to allow for the selection of output type: either MET or CSV\n",
    "    \n",
    " TODO:\n",
    "    1. Implement a new functionality in APSIM that automatically executes this code by only providing lat and lon values (and generating a MET)\n",
    "    2. Use AutoComplete package to help in commandline params: https://github.com/kislyuk/argcomplete.\n",
    "    3. Allow for the extraction of climate data from any other gridded climate data source as long as it is encoded in NETCDF4. Example: allowing the user to pass a parameter for the cloud (S3 or other) location of their data\n",
    "    4. Extend output formats to generate input climate data for other crop models (DSSAT, STICS)\n",
    "    5. Implement MultiProcessing to allow for the parallelization of workloads\n",
    "\n",
    "'''\n",
    "\n",
    "class Arguments():\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.parser = argparse.ArgumentParser(\n",
    "            description=\"Bestiapop Climate Data Extractor\"\n",
    "            )\n",
    "\n",
    "        self.parser.add_argument(\n",
    "            \"-a\", \"--action\",\n",
    "            help=\"The type of operation to want to perform: download-silo-file (it will only download a particular SILO file from S3 to your local disk), convert-nc4-to-met (it will only convert a local or S3 file from NC4 format to MET), convert-nc4-to-csv(it will only convert a local or S3 file from NC4 format to CSV), download-and-convert-to-met (combines the first two actions)\",\n",
    "            type=str,\n",
    "            choices=[\"download-silo-file\", \"convert-nc4-to-met\", \"convert-nc4-to-csv\", \"generate-met-file\"],\n",
    "            default=\"convert-nc4-to-csv\",\n",
    "            required=True\n",
    "            )\n",
    "\n",
    "        self.parser.add_argument(\n",
    "            \"-y\", \"--year-range\",\n",
    "            help=\"This option determines what is the range of years that you want to download data for. It can be a single year (-y 2015) or a range, using a dash in between (-y 2000-2015)\",\n",
    "            type=str,\n",
    "            default=\"\",\n",
    "            required=True\n",
    "            )\n",
    "\n",
    "        self.parser.add_argument(\n",
    "            \"-c\", \"--climate-variable\",\n",
    "            help=\"The climate variable you want to download data for. To see all variables: https://www.longpaddock.qld.gov.au/silo/about/climate-variables/. You can also specify multiple variables separating them with spaces, example: \"\"daily_rain radiation min_temp max_temp\"\"\",\n",
    "            type=self.climate_variable_list,\n",
    "            default=\"daily_rain\",\n",
    "            required=True\n",
    "            )\n",
    "\n",
    "        self.parser.add_argument(\n",
    "            \"-lat\", \"--latitude-range\",\n",
    "            help=\"The latitude range to download data from the grid to a decimal degree, separated by a \"\"space\"\", in increments of 0.05. It also accepts single values. Examples: -lat \"\"-40.85 -40.90\"\" \\n -lat \"\"30.10 33\"\" \\n -lat -41\",\n",
    "            type=self.csv_list,\n",
    "            default=None,\n",
    "            required=False\n",
    "            )\n",
    "\n",
    "        self.parser.add_argument(\n",
    "            \"-lon\", \"--longitude-range\",\n",
    "            help=\"The longitude range to download data from the grid to a decimal degree, separated by a \"\"space\"\", in increments of 0.05. It also accepts single values. Examples: -lon \"\"145.45 145.5\"\" \\n -lon \"\"145.10 146\"\" \\n -lon 145\",\n",
    "            type=self.csv_list,\n",
    "            required=False\n",
    "            )\n",
    "\n",
    "        self.parser.add_argument(\n",
    "            \"-i\", \"--input-path\",\n",
    "            help=\"For \"\"convert-nc4-to-met\"\" and \"\"convert-nc4-to-csv\"\", the file or folder that will be ingested as input in order to extract the specified data. Example: -i \"\"C:\\\\some\\\\folder\\\\2015.daily_rain.nc\"\". When NOT specified, the tool assumes it needs to get the data from the cloud.\",\n",
    "            type=str,\n",
    "            default=os.getcwd(),\n",
    "            required=False\n",
    "            )\n",
    "\n",
    "        self.parser.add_argument(\n",
    "            \"-o\", \"--output-directory\",\n",
    "            help=\"This argument is required and represents the directory that we will use to: (a) stage the netCDF4 files as well as save any output (MET, CSV, etc.) or (b) collect any .nc files when you have already downloaded them for conversion to CSV or MET. If no folder is passed in, the current directory is assumed to the right directory. Examples: (1) download files to a local disk: -o \"\"C:\\\\some\\\\folder\\\\path\"\"\",\n",
    "            type=str,\n",
    "            default=os.getcwd(),\n",
    "            required=True\n",
    "            )\n",
    "\n",
    "        self.parser.add_argument(\n",
    "            \"-ot\", \"--output-type\",\n",
    "            help=\"This argument will tell the script whether you want the output file to be in CSV or MET format\",\n",
    "            type=str,\n",
    "            choices=[\"met\", \"csv\"],\n",
    "            default=\"met\",\n",
    "            required=False\n",
    "            )\n",
    "\n",
    "        self.pargs = self.parser.parse_args()\n",
    "\n",
    "    def csv_list(self, string):\n",
    "        # Adding our own parser for comma separated values\n",
    "        # since Argparse interprets them as multiple values and complains\n",
    "        if \" \" in string:\n",
    "            return [float(x) for x in string.split()]\n",
    "        else:\n",
    "            return [float(string)]\n",
    "\n",
    "    def climate_variable_list(self, string):\n",
    "        # Adding our own parser for comma separated values\n",
    "        # since Argparse interprets them as multiple values and complains\n",
    "        if \" \" in string:\n",
    "            return [str(x) for x in string.split()]\n",
    "        else:\n",
    "            return string\n",
    "\n",
    "    def get_args(self):\n",
    "        return self.pargs\n",
    "\n",
    "class SILO():\n",
    "\n",
    "    def __init__(self, logger, action, outputpath, output_type, inputpath, variable_short_name, year_range, lat_range, lon_range):\n",
    "\n",
    "        # Initializing variables\n",
    "        self.action = action\n",
    "        self.logger = logger\n",
    "        self.logger.info('Initializing {}'.format(__name__))\n",
    "        self.outputdir = Path(outputpath)\n",
    "        self.output_type = output_type\n",
    "        self.variable_short_name = variable_short_name\n",
    "        \n",
    "        # Check whether a year range with \"-\" was provided for the year.\n",
    "        # If this is the case, generate a list out of it\n",
    "        # TODO: handle lists of discontinuous year ranges like [1999-2001,2005-2019]\n",
    "        if \"-\" in year_range:\n",
    "            first_year = int(year_range.split(\"-\")[0])\n",
    "            last_year = int(year_range.split(\"-\")[1]) + 1\n",
    "            year_range = np.arange(first_year,last_year,1)\n",
    "        else:\n",
    "            year_range = [int(year_range)]\n",
    "\n",
    "        self.year_range = year_range\n",
    "\n",
    "        # Check whether a lat and lon range separated by a space was provided.\n",
    "        # If this is the case, generate a list out of it\n",
    "        # NOTE: for some reason I get a list within a list from the argparse...\n",
    "        if lat_range:\n",
    "            if len(lat_range) > 1:\n",
    "                if lat_range[0] < 0 and lat_range[1] < 0:\n",
    "                    if lat_range[0] > lat_range[1]:\n",
    "                        # We are clearly dealing with negative numbers\n",
    "                        # User has mistakenly swapped the order of numbers\n",
    "                        # we need to silently swap them back\n",
    "                        first_lat = lat_range[1]\n",
    "                        last_lat = lat_range[0]\n",
    "                    else:\n",
    "                        first_lat = lat_range[0]\n",
    "                        last_lat = lat_range[1]\n",
    "                lat_range = np.arange(first_lat,last_lat,0.05).round(decimals=2)\n",
    "            self.lat_range = lat_range\n",
    "\n",
    "        if lon_range:\n",
    "            if len(lon_range) > 1:\n",
    "                if lon_range[0] > lon_range[1]:\n",
    "                    # User has mistakenly swapped the order of numbers\n",
    "                    # we need to silently swap them back\n",
    "                    first_lon = lon_range[1]\n",
    "                    last_lon = lon_range[0]\n",
    "                else:\n",
    "                    first_lon = lon_range[0]\n",
    "                    last_lon = lon_range[1]\n",
    "                lon_range = np.arange(first_lon,last_lon,0.05).round(decimals=2)\n",
    "            self.lon_range = lon_range\n",
    "\n",
    "        # Validate input directory\n",
    "        # TODO\n",
    "\n",
    "        # Validate output directory\n",
    "        if self.outputdir.is_dir() == True:\n",
    "            if self.outputdir.exists() == False:\n",
    "                self.logger.info('{} does not exist, creating it...'.format(self.outputdir))\n",
    "                self.outputdir.mkdir(parents=True, exist_ok=True)\n",
    "        elif (self.outputdir.is_file() == True) and (self.outputdir.exists() == False):\n",
    "            self.logger.error('File {} does not exist'.format(self.outputdir))\n",
    "        else:\n",
    "            self.logger.error('{} is not a folder, please provide a folder path'.format(self.outputdir))\n",
    "            sys.exit(1)\n",
    "  \n",
    "    def process_records(self, action):\n",
    "        # Let's check what's inside the \"action\" variable and invoke the corresponding function\n",
    "        if action == \"download-silo-file\":\n",
    "            self.logger.info('Action {} invoked'.format(action))\n",
    "            for year in self.year_range:\n",
    "                self.logger.info('Downloading SILO file for year {}'.format(year))\n",
    "                self.download_file_from_silo_s3(year, self.variable_short_name, self.outputdir)\n",
    "\n",
    "        elif action == \"convert-nc4-to-met\":\n",
    "            self.logger.info('Action {} not implemented yet'.format(action))\n",
    "\n",
    "        elif action == \"convert-nc4-to-csv\":\n",
    "            self.logger.info('Converting files to CSV format')\n",
    "            # 1. Let's invoke generate_climate_dataframe with the appropriate options\n",
    "            self.generate_climate_dataframe(year_range=self.year_range,\n",
    "                                        variable_short_name=self.variable_short_name, \n",
    "                                        lat_range=self.lat_range,\n",
    "                                        lon_range=self.lon_range,\n",
    "                                        outputdir=self.outputdir,\n",
    "                                        download_files=False,\n",
    "                                        output_to_file=True,\n",
    "                                        output_format=\"CSV\")\n",
    "\n",
    "        elif action == \"generate-met-file\":\n",
    "            self.logger.info('Downloading data and converting to {} format'.format(self.output_type))\n",
    "            # 1. Let's invoke generate_climate_dataframe with the appropriate options\n",
    "            self.generate_climate_dataframe(year_range=self.year_range,\n",
    "                                        variable_short_name=self.variable_short_name, \n",
    "                                        lat_range=self.lat_range,\n",
    "                                        lon_range=self.lon_range,\n",
    "                                        outputdir=self.outputdir,\n",
    "                                        download_files=False,\n",
    "                                        output_to_file=True,\n",
    "                                        output_format=\"MET\")\n",
    "\n",
    "    def load_cdf_file(self, sourcepath, data_category, load_from_s3=True, year=None):\n",
    "\n",
    "        # This function loads the \".nc\" file using the xarray library and\n",
    "        # stores a pointer to it in \"data_dict\"\n",
    "\n",
    "        # Let's first check whether a source file was passed in, otherwise\n",
    "        # assume we need to fetch from the cloud\n",
    "        if load_from_s3 == True:\n",
    "            silo_file = \"silo-open-data/annual/{}/{}.{}.nc\".format(data_category, year, data_category)\n",
    "            fs_s3 = s3fs.S3FileSystem(anon=True)\n",
    "            remote_file_obj = fs_s3.open(silo_file, mode='rb')\n",
    "            DS_data_handle = xr.open_dataset(remote_file_obj, engine='h5netcdf')\n",
    "            self.logger.debug('Loaded netCDF4 file {} from Amazon S3'.format(silo_file))\n",
    "        \n",
    "        else:\n",
    "            # This function expects that we will pass the value series\n",
    "            # we are looking for in the \"data_category\" parameter\n",
    "            # So if we want the function to return all values for \n",
    "            # rain we shall call the function as:\n",
    "            # load_file(sourcepath, sourcefile, 'daily_rain')\n",
    "            self.logger.info('Loading netCDF4 file {}'.format(sourcepath))\n",
    "            DS_data_handle = xr.open_dataset(sourcepath)\n",
    "        \n",
    "        # Extracting the \"year\" from within the file itself.\n",
    "        # For this we get a sample of the values and then \n",
    "        # convert the first value to a year. Assuming we are dealing\n",
    "        # with single year files as per SILO S3 files, this shouldn't\n",
    "        # represent a problem\n",
    "        DS_sample = DS_data_handle.time.head().values[1]\n",
    "        data_year = DS_sample.astype('datetime64[Y]').astype(int) + 1970\n",
    "        \n",
    "        # Storing the pointer to the data and the year in a dict\n",
    "        data_dict = {\n",
    "            \"value_array\": DS_data_handle, \n",
    "            \"data_year\": data_year,            \n",
    "        }\n",
    "        \n",
    "        # returning our dictionary with relevant values\n",
    "        return data_dict\n",
    "\n",
    "    def download_file_from_silo_s3(self, year, variable_short_name, output_path = Path().cwd()):\n",
    "        # This function connects to the public S3 site for SILO and downloads the specified file\n",
    "        # For a list of variables to use in \"variable_short_name\" see\n",
    "        # https://silo.longpaddock.qld.gov.au/climate-variables\n",
    "        # Most common are: daily_rain, max_temp, min_temp\n",
    "        # Example, call the function like: download_file_from_silo_s3(2011, \"daily_rain\")\n",
    "        # The above will save to the current directory, however, you can also pass\n",
    "        # your own like: download_file_from_silo_s3(2011,'daily_rain','C:\\\\Downloads\\\\SILO\\2011')\n",
    "\n",
    "        # We use TQDM to show a progress bar of the download status\n",
    "      \n",
    "        filename = str(year) + \".\" + variable_short_name + \".nc\"\n",
    "        url = 'https://s3-ap-southeast-2.amazonaws.com/silo-open-data/annual/{}/{}'.format(variable_short_name, filename)\n",
    "\n",
    "        # Get pointer to URL\n",
    "        req = requests.get(url, stream=True)\n",
    "\n",
    "        # Set initial file size and total file size\n",
    "        first_byte = 0\n",
    "        total_file_size = int(req.headers.get(\"Content-Length\", 0))\n",
    "        \n",
    "        if first_byte >= total_file_size:\n",
    "            return total_file_size\n",
    "\n",
    "        progressbar = tqdm(\n",
    "                            total=total_file_size,\n",
    "                            initial=first_byte,\n",
    "                            unit='B',\n",
    "                            ascii=True,\n",
    "                            unit_scale=True,\n",
    "                            desc=url.split('/')[-1])\n",
    "        \n",
    "        # Write file in chunks\n",
    "        # Let's first check whether the file has already been downloaded\n",
    "        # if it has, let's return without downloading it again\n",
    "        output_file = output_path/filename\n",
    "        if output_file.is_file() == True:\n",
    "            if output_file.exists() == True:\n",
    "                self.logger.info('File {} already exists. Skipping download...'.format(output_file))\n",
    "                return\n",
    "\n",
    "        chunk_size = 1024\n",
    "        with open(output_file, 'ab') as f:\n",
    "            self.logger.info('Downloading file {}...'.format(output_file))\n",
    "            for chunk in req.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    progressbar.update(1024)\n",
    "                    \n",
    "        progressbar.close()\n",
    "\n",
    "    def get_values_from_array(self, lat, lon, value_array, file_year, variable_short_name):\n",
    "        # This function will use xarray to extract a slice of time data for a combination of lat and lon values\n",
    "\n",
    "        # Checking if this is a leap-year  \n",
    "        if (( file_year%400 == 0) or (( file_year%4 == 0 ) and ( file_year%100 != 0))):\n",
    "            days = np.arange(0,366,1)\n",
    "        else: \n",
    "            days = np.arange(0,365,1)\n",
    "\n",
    "        # Using a list comprehension to capture all daily values for the given year and lat/lon combinations\n",
    "        # We round values to a single decimal\n",
    "        self.logger.debug(\"Reading array data with xarray\")\n",
    "        data_values = [np.round(x, decimals=1) for x in value_array[variable_short_name].sel(lat=lat, lon=lon).values]\n",
    "\n",
    "        # We have captured all 365 or 366 values, however, they could all be NaN (non existent)\n",
    "        # If this is the case, skip it\n",
    "        # NOTE: we could have filtered this in the list comprehension above, however\n",
    "        # we chose to do it here for code readability\n",
    "        data_values = [x for x in data_values if np.isnan(x) != True]\n",
    "\n",
    "        # we need to get the total amount of values collected\n",
    "        # if there was \"NO\" data available for all days under a particular combination\n",
    "        # of lat & lon, then the total values collected should equal \"0\"\n",
    "        # (meaning, there was no data for that point in the grid)\n",
    "        # If this is the case, then the function will simply return with\n",
    "        # a \"no_values\"\n",
    "        if len(data_values) == 0:\n",
    "            # DEBUG - ERASE\n",
    "            print(\"THERE ARE NO VALUES FOR LAT {} LON {} VARIABLE {}\".format(lat, lon, variable_short_name))\n",
    "            raise ValueError('No data for the lat & lon combination provided')\n",
    "      \n",
    "        # now we need to fill a PANDAS DataFrame with the lists we've been collecting\n",
    "        pandas_dict_of_items = {'days': days,\n",
    "                                variable_short_name: data_values}\n",
    "      \n",
    "        df = pd.DataFrame.from_dict(pandas_dict_of_items)\n",
    "        \n",
    "        # making the julian day match the expected\n",
    "        df['days'] += 1\n",
    "        \n",
    "        # adding a column with the \"year\" to the df\n",
    "        # so as to prepare it for export to other formats (CSV, MET, etc.)\n",
    "        df.insert(0, 'year', file_year)\n",
    "        df.insert(0, 'lat', lat)\n",
    "        df.insert(0, 'lon', lon)\n",
    "      \n",
    "        return df\n",
    "\n",
    "    def generate_climate_dataframe(self, year_range, variable_short_name, lat_range, lon_range, outputdir, download_files=False, load_from_s3=True, output_to_file=True, output_format=\"CSV\"):\n",
    "\n",
    "        '''\n",
    "        Creation of the DataFrame and Files\n",
    "        ===================================\n",
    "\n",
    "        We will iterate through each \"latitude\" value and, \n",
    "        within this loop, we will iterate through all the different \n",
    "        \"longitude\" values for a given year. Results for each year\n",
    "        are collected inside the \"met_df\" with \"met_df.append\"\n",
    "        At the end, it will output a file with all the contents if\n",
    "        \"output_to_file=True\" (by default it is \"True\")\n",
    "        '''\n",
    "        self.logger.debug('Generating DataFrames')\n",
    "\n",
    "        # let's first create an empty df to store \n",
    "        # all data for a given variable-year-lat-lon combination\n",
    "        met_df = pd.DataFrame()\n",
    "\n",
    "        # empty df to append all the met_df to\n",
    "        total_met_df = pd.DataFrame()\n",
    "\n",
    "        # Loading and/or Downloading the files\n",
    "        for climate_variable in tqdm(variable_short_name, ascii=True, desc=\"Climate Variable\"):\n",
    "\n",
    "            self.logger.debug('Processing data for variable {}'.format(climate_variable))\n",
    "\n",
    "            for year in tqdm(year_range, ascii=True, desc=\"Year\"):\n",
    "\n",
    "                self.logger.debug('Processing data for year {}'.format(year))\n",
    "\n",
    "                # should we download the file first?\n",
    "                if download_files == True:\n",
    "                    self.logger.debug('Attempting to download files')\n",
    "                    self.download_file_from_silo_s3(year, climate_variable, outputdir)\n",
    "\n",
    "                # Opening the target CDF database\n",
    "                # We need to check:\n",
    "                # (1) should we fetch the data directly from AWS S3 buckets\n",
    "                # (2) if files should be fetched locally, whether the user passed a directory with multiple files or just a single file to process.\n",
    "                if load_from_s3 == True:\n",
    "                    data = self.load_cdf_file(None, climate_variable, load_from_s3=True, year=year)\n",
    "                else:\n",
    "                    if outputdir.is_dir() == True:\n",
    "                        sourcefile = str(year) + \".\" + climate_variable + \".nc\"\n",
    "                        sourcepath = outputdir/sourcefile\n",
    "                    elif outputdir.is_file() == True:\n",
    "                        sourcepath = outputdir\n",
    "\n",
    "                    if sourcepath.exists() == False:\n",
    "                        self.logger.error('Could not find file {}. Please make sure you have downloaded the required netCDF4 files in the format \"year.variable.nc\" to the input directory. Skipping...'.format(sourcepath))\n",
    "                        continue\n",
    "\n",
    "                    data = self.load_cdf_file(sourcepath, climate_variable)\n",
    "            \n",
    "                # Now iterating over lat and lon combinations\n",
    "                # Each year-lat-lon matrix generates a different file\n",
    "                \n",
    "                for lat in tqdm(lat_range, ascii=True, desc=\"Latitude\"):\n",
    "\n",
    "                    for lon in lon_range:\n",
    "\n",
    "                        file_year = data['data_year']\n",
    "\n",
    "                        self.logger.debug('Processing Variable {} - Lat {} - Lon {} for Year {}'.format(climate_variable, lat, lon, file_year))\n",
    "\n",
    "                        # here we are checking whether the get_values_from_cdf function\n",
    "                        # returns with a ValueError (meaning there were no values for\n",
    "                        # that particular lat & long combination). If it does return\n",
    "                        # with an error, we skip this loop and don't produce any output files\n",
    "                    \n",
    "                        try:\n",
    "                            var_year_lat_lon_df = self.get_values_from_array(lat, lon, data['value_array'], file_year, climate_variable)\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                        \n",
    "                        # Should we generate any file output for this var-year-lat-lon iteration?\n",
    "                        if output_to_file == True:\n",
    "                                    \n",
    "                            # Should we output using CSV file format?\n",
    "                            if output_format == \"CSV\":\n",
    "                                # let's build the name of the file based on the value of the \n",
    "                                # first row for latitude, the first row for longitude and then \n",
    "                                # the year (obtained from the name of the file with file_year = int(sourcefile[:4]))\n",
    "                                # Note: there is a better method for obtaining this by looking at the\n",
    "                                # \"time\" variable, see here below:\n",
    "\n",
    "                                if outputdir.is_dir() == True:\n",
    "                                    csv_file_name = '{}-{}.{}-{}.csv'.format(climate_variable, file_year, lat, lon)\n",
    "                                    self.logger.debug('Writting CSV file {} to {}'.format(csv_file_name, outputdir))\n",
    "                                    full_output_path = outputdir/csv_file_name\n",
    "                                    var_year_lat_lon_df.to_csv(full_output_path, sep=',', index=False, mode='a', float_format='%.2f')\n",
    "                        \n",
    "                        # \"reset\" the var_year_lat_lon_df back to zero.\n",
    "                        total_met_df = total_met_df.append(var_year_lat_lon_df)\n",
    "                        var_year_lat_lon_df = pd.DataFrame()\n",
    "\n",
    "        # Should we generate any file output for this var-year-lat-lon iteration?\n",
    "        if output_to_file == True:\n",
    "            \n",
    "            #Rename variables\n",
    "            total_met_df = total_met_df.rename(columns={\"days\": \"day\",\"daily_rain\": \"rain\",'min_temp':'mint','max_temp':'maxt','radiation':'radn'})\n",
    "            total_met_df = total_met_df.groupby(['lon', 'lat', 'year', 'day'])['radn', 'maxt', 'mint', 'rain'].sum().reset_index()\n",
    "            \n",
    "            self.logger.info(\"Proceeding to the generation of MET files\")\n",
    "\n",
    "            for lat in tqdm(lat_range, ascii=True, desc=\"Latitude\"):\n",
    "                \n",
    "                for lon in tqdm(lon_range, ascii=True, desc=\"Latitude\"):\n",
    "\n",
    "                    met_slice_df = total_met_df[(total_met_df.lon == lon) & (total_met_df.lat == lat)]\n",
    "                    del met_slice_df['lat']\n",
    "                    del met_slice_df['lon']\n",
    "\n",
    "                    if self.output_type == \"met\":\n",
    "                        self.generate_met(outputdir, met_slice_df, lat, lon)\n",
    "                    \n",
    "                    elif self.output_type == \"csv\":\n",
    "                        full_output_path = outputdir/'{}-{}.csv'.format(lat, lon)\n",
    "                        met_slice_df.to_csv(full_output_path, sep=\",\", index=False, mode='w', float_format='%.2f')\n",
    "\n",
    "                    else:\n",
    "                        self.logger.info(\"Output not yet implemented\")\n",
    "\n",
    "            generate_final_csv = False\n",
    "            if generate_final_csv == True:\n",
    "                # Creating final CSV file\n",
    "                csv_file_name = 'mega_final_data_frame.csv'\n",
    "                self.logger.info('Writting CSV file {} to {}'.format(csv_file_name, outputdir))\n",
    "                full_output_path = outputdir/csv_file_name\n",
    "                total_met_df.to_csv(full_output_path, sep=',', na_rep=np.nan, index=False, mode='w', float_format='%.2f')\n",
    "\n",
    "    def generate_met(self, outputdir, met_dataframe, lat, lon):\n",
    "\n",
    "        # Creating final MET file\n",
    "\n",
    "        # Setting up Jinja2 Template for final MET file if required\n",
    "        # Text alignment looks weird here but it must be left this way for proper output\n",
    "        met_file_j2_template = '''[weather.met.weather]\n",
    "!station number={{ lat }}-{{ lon }}\n",
    "Latitude={{ lat }}\n",
    "Longitude={{ lon }}\n",
    "tav={{ tav }}\n",
    "amp={{ amp }}\n",
    "\n",
    "year day radn maxt mint rain\n",
    "() () (MJ^m2) (oC) (oC) (mm)\n",
    "{{ vardata }}\n",
    "        '''\n",
    "\n",
    "        j2_template = Template(met_file_j2_template)\n",
    "\n",
    "        # Initialize a string buffer to receive the output of df.to_csv in-memory\n",
    "        df_output_buffer = io.StringIO()\n",
    "\n",
    "        # Save data to a buffer (same as with a regular file but in-memory):\n",
    "        met_dataframe.to_csv(df_output_buffer, sep=\" \", header=False, na_rep=\"NaN\", index=False, mode='w', float_format='%.1f')\n",
    "\n",
    "        # Get values from buffer\n",
    "        # Go back to position 0 to read from buffer\n",
    "        # Replace get rid of carriage return or it will add an extra new line between lines\n",
    "        df_output_buffer.seek(0)\n",
    "        met_df_text_output = df_output_buffer.getvalue()\n",
    "        met_df_text_output = met_df_text_output.replace(\"\\r\\n\", \"\\n\")\n",
    "        \n",
    "        # Calculate here the tav, amp values\n",
    "        # TODO\n",
    "        # Calculate amp\n",
    "\n",
    "        # Get the months as a column\n",
    "        met_dataframe['cte'] = 1997364\n",
    "        met_dataframe['day2'] = met_dataframe['day']+met_dataframe['cte']\n",
    "        met_dataframe['date'] = (pd.to_datetime((met_dataframe.day2 // 1000)) + pd.to_timedelta(met_dataframe.day2 % 1000, unit='D'))\n",
    "        met_dataframe['month'] = met_dataframe.date.dt.month\n",
    "        month=met_dataframe.loc[:,'month']\n",
    "\n",
    "        met_dataframe['tmean'] = met_dataframe[['maxt', 'mint']].mean(axis=1)\n",
    "        tmeanbymonth = met_dataframe.groupby(month)[[\"tmean\"]].mean()\n",
    "        maxmaxtbymonth = tmeanbymonth.loc[tmeanbymonth['tmean'].idxmax()].round(decimals=5)\n",
    "        minmaxtbymonth = tmeanbymonth.loc[tmeanbymonth['tmean'].idxmin()].round(decimals=5)\n",
    "        amp = maxmaxtbymonth-minmaxtbymonth\n",
    "        amp = list(amp)[0]\n",
    "\n",
    "        # Calculate tav\n",
    "        tav = tmeanbymonth.mean().tmean.round(decimals=5)\n",
    "        \n",
    "        in_memory_met = j2_template.render(lat=lat, lon=lon, tav=tav, amp=amp, vardata=met_df_text_output)\n",
    "        df_output_buffer.close()\n",
    "\n",
    "        full_output_path = outputdir/'{}-{}.met'.format(lat, lon)\n",
    "        with open(full_output_path, 'w+') as f:\n",
    "            self.logger.info('Writting MET file {}'.format(full_output_path))\n",
    "            f.write(in_memory_met)\n",
    "\n",
    "    def main():\n",
    "      # Instantiating the arguments class\n",
    "        args = Arguments(sys.argv)\n",
    "        pargs = args.get_args()\n",
    "\n",
    "      # Setup logging\n",
    "      # We need to pass the \"logger\" to any Classes or Modules that may use it \n",
    "      # in our script\n",
    "        try:\n",
    "            import coloredlogs\n",
    "            logger = logging.getLogger('POPBEAST')\n",
    "            coloredlogs.install(fmt='%(asctime)s - %(name)s - %(message)s', level=\"DEBUG\", logger=logger)\n",
    "\n",
    "        except ModuleNotFoundError:\n",
    "            logger = logging.getLogger('POPBEAST')\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(message)s')\n",
    "            console_handler = logging.StreamHandler()\n",
    "            console_handler.setFormatter(formatter)\n",
    "            console_handler.setLevel(logging.DEBUG)\n",
    "            logger.addHandler(console_handler)\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "          # Capturing start time for debugging purposes\n",
    "            st = datetime.now()\n",
    "            starttime = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "            logger.info(\"Starting POPBEAST Climate Automation Framework\")\n",
    "\n",
    "          # Grab an instance of the SILO class\n",
    "            silo_instance = SILO(logger, pargs.action, pargs.output_directory, pargs.output_type, pargs.input_path, pargs.climate_variable, pargs.year_range, pargs.latitude_range, pargs.longitude_range)\n",
    "          # Start to process the records\n",
    "            silo_instance.process_records(pargs.action)\n",
    "\n",
    "          # Capturing end time for debugging purposes\n",
    "            et = datetime.now()\n",
    "            endtime = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "            hours, remainder = divmod((et-st).seconds, 3600)\n",
    "            minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "            logger.info(\"Finished this unit of work\")\n",
    "            logger.info('Workload took: \\x1b[47m \\x1b[32m{} hours / {} minutes / {} seconds \\x1b[0m \\x1b[39m'.format(hours,minutes,seconds))\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            try:\n",
    "                main()\n",
    "                sys.exit()\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\" + \"I've been interrupted by a mortal\" + \"\\n\\n\")\n",
    "                sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
